{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebParsing\n",
    "\n",
    "В этом задании требуется обкачать интернет-магазин компьютерных игр [\"GG.DEALS\"](https://gg.deals/) с использованием библиотек [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) и/или [lxml](https://lxml.de/). Ваша программа должна скачать информацию о самых [рейтинговых](https://gg.deals/games/?sort=metascore&type=1) 300 играх магазина. \n",
    "\n",
    "\n",
    "\n",
    "## Общий подход к решению задачи\n",
    "\n",
    "Задачу условно можно разделить на два этапа. На первом этапе требуется получить ссылки на все нужные игры из раздела, на втором – получить информацию о каждой из игр.\n",
    "\n",
    "### Этап 1. Получение ссылок на игры\n",
    "\n",
    "Для получения всех ссылок на игры нужно обкачать первые страницы [раздела](https://gg.deals/games/?sort=metascore&type=1). Переключаясь между страницами раздела можно заметить, как меняется URL страницы (появляется параметр `page`). Варьируя это значение в диапазоне можно получить ссылки на все требуемые страницы.\n",
    "\n",
    "### Этап 2. Получение информации об игре\n",
    "\n",
    "Требуется извлечь следующие элементы:\n",
    "\n",
    "0. поле \"url\" – url страницы из адресной строки браузера;\n",
    "1. поле \"name\" – название игры;\n",
    "2. поле \"image\" – ссылка на постер игры; поле \"market_url\" – ссылка на игру в оригинальном магазине (см. View On Steam, например);\n",
    "3. поля \"wishlist_count\", \"alert_count\", \"owners_count\" – значения соответвующих счетчиков.\n",
    "4. группы полей, если имеются:\n",
    "\t* \"release_date\" – дата релиза (выхода) игры;\n",
    "\t* \"developer\" – разработчик игры;\n",
    "\t* \"metacritic_score\" – рейтинг Metascore;\n",
    "\t* \"user_score\" – рейтинг Userscore;\n",
    "\t* \"review_label\", \"review_positive_pctg\", \"review_count\" – общий пользовательский вердикт (например, Very Positive), доля позитивных обзоров, общее число обзоров на игру;\n",
    "\t* \"genres\" – список жанров игры;\n",
    "\t* \"tags\" – список тегов игры;\n",
    "\t* \"features\" – список особенностей игры.\n",
    "5. поле \"dlcs\" – список ссылок на DLC (дополнения) к игре, поле \"packs\" – список ссылок на Packs (расширенные версии игр); списки могут быть пустыми;\n",
    "6. поле \"pc_systems\" – список поддерживаемых ОС компьютеров;\n",
    "7. поле \"price_history\" – список цен на игру в оригинальных магазинах (голубая линия) за весь имеющийся период. \n",
    " \n",
    "Список цен на игру должен представлять собой питоновский список словарей, каждый словарь должен иметь три поля:\n",
    "* \"ts\" – время изменения цены;\n",
    "* \"price\" – новая цена (в рублях);\n",
    "* \"shop\" – имя магазина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from lxml import etree, html as lhtml\n",
    "from bs4 import BeautifulSoup\n",
    "import ast\n",
    "\n",
    "from multiprocessing.dummy import Pool, Queue\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstPart = 'https://gg.deals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_make(dc):\n",
    "    genre = dc.xpath('//a[contains(@href, \"/games/?genre=\")]/text()')\n",
    "    if (genre):\n",
    "        return [i for i in genre if(i.isalpha())]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_make(ind):\n",
    "    \n",
    "    if (ind):\n",
    "        headers_dict = {\"x-requested-with\": \"XMLHttpRequest\"}\n",
    "        res = requests.get(\"https://gg.deals/ru/games/chartHistoricalData/{}/?hideKeyshops=0\".format(ind),\n",
    "                           headers = headers_dict)\n",
    "        part = res.text.split('\"deals\":')\n",
    "        if (part and len(part) >=2 ):\n",
    "            part2 = part[1].split('\"offers\":')\n",
    "            part3 = part2[0][:-1].replace('\"x\"','\"ts\"').replace('\"y\"','\"price\"')\n",
    "            if (part3 != '[]'):\n",
    "                ans = json.loads(part3)\n",
    "                for i in ans:\n",
    "                    i.pop('name')\n",
    "                return ans\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_make(dc):\n",
    "    res = dc.xpath('//img[contains(@class, \"image-game\")]/@srcset')\n",
    "    if (res):\n",
    "        image = res[0]\n",
    "        image = image.split(',')\n",
    "        return [i.split(' ')[0] for i in image]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def none_check(parent):\n",
    "    if parent:\n",
    "        return True\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlcs_packs(ind, type_d):\n",
    "    headers_dict = {\"x-requested-with\": \"XMLHttpRequest\"}\n",
    "    page = requests.get('https://gg.deals/ru/games/relations/{}/?type={}&offset=0&hideKeyshops=0'.format(ind, type_d),\n",
    "                            headers=headers_dict)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    res = soup.find_all('a', class_ = \"game-info-title\")\n",
    "    if (res):\n",
    "        return [firstPart + i.attrs['href'] for i in res]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page(url):\n",
    "    try:\n",
    "        r = get_page(url)\n",
    "\n",
    "        dc = lhtml.fromstring(r.text)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        name = dc.xpath('//span[@itemprop=\"name\"]/text()')\n",
    "        image = image_make(dc)\n",
    "        market_url = soup.find('a', class_ = 'score-grade')\n",
    "        wishlist = soup.find_all(\"span\", class_=\"count\")\n",
    "        dlms = soup.find('section', id = 'game-dlcs')\n",
    "        packs = soup.find('section', id = 'game-packs')\n",
    "        release_date = dc.xpath('//p[contains(@class, \"game-info-details-content\")]/text()')\n",
    "        metacritic = soup.find('a', class_ = 'score-circle score-metascore')\n",
    "        user = soup.find('a', class_ = 'score-circle score-userscore')\n",
    "        review = soup.find('span', class_ = 'reviews-label')\n",
    "        tags = dc.xpath('//a[contains(@href, \"/games/?tag=\")]/text()') \n",
    "        features = dc.xpath('//a[contains(@href, \"/games/?feature=\")]/text()')\n",
    "        check = soup.find('div', class_ = 'game-info-actions')\n",
    "        p = soup.find('div', class_ = \"game-requirements-tabs\")\n",
    "        if (none_check(check)):\n",
    "            a = check.find('div',\n",
    "                            class_ = lambda s: s and s.startswith('game-collection-actions')).attrs['data-counters-url']\n",
    "            ind = a[:-1].split('/')[-1]\n",
    "        else:\n",
    "            ind = None\n",
    "\n",
    "        if (p):\n",
    "            pc = [i.text for i in p.find_all('li', class_ = lambda s: s and s.endswith('menu-item'))] \n",
    "        else:\n",
    "            pc = None\n",
    "\n",
    "        game_info = {\n",
    "            'url': url, \n",
    "            'name': name[-1] if name else None,\n",
    "            'image': image[0] if (image) else None,\n",
    "            \"market_url\":  none_check(market_url) and market_url.attrs['href'],\n",
    "            \"wishlist_count\": none_check(wishlist) and int(wishlist[0].text),\n",
    "            \"alert_count\": none_check(wishlist) and int(wishlist[-3].text),\n",
    "            \"owners_count\": none_check(wishlist) and int(wishlist[-1].text),\n",
    "            \"release_date\": none_check(release_date) and release_date[0],\n",
    "            \"developer\": none_check(release_date) and release_date[1],\n",
    "            \"metacritic_score\": none_check(metacritic) and float(metacritic.find('span', class_ = 'overlay').text),\n",
    "            \"user_score\": none_check(user) and float(user.find('span', class_ = 'overlay').text),\n",
    "            \"review_label\": none_check(review) and\n",
    "                                        (review.text).split(' (', maxsplit = 1)[0],\n",
    "            \"review_positive_pctg\": none_check(review) and\n",
    "                                       int(review.attrs['title'].split('%', maxsplit = 1)[0]),\n",
    "            \"review_count\": none_check(review) and\n",
    "                                       int((review.text).split(' (', maxsplit = 1)[1][:-1].replace(',','')),\n",
    "            \"genres\": genre_make(dc),\n",
    "            \"tags\": tags if tags else None,\n",
    "            \"features\": features if features else None,\n",
    "            \"dlcs\": dlcs_packs(ind,'dlc'),\n",
    "            \"packs\": dlcs_packs(ind, 'packs'),\n",
    "            \"pc_systems\": pc,\n",
    "            \"price_history\": dict_make(ind)\n",
    "        }\n",
    "        \n",
    "        filtered = dict(filter(lambda item: item[1] is not None, game_info.items()))\n",
    "        return filtered\n",
    "    except Exception as ex:\n",
    "        template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "        message = template.format(type(ex).__name__, ex.args)\n",
    "        print(message)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url, attempts = 5):\n",
    "    for i in range(attempts):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        else:\n",
    "            print(response.status_code)\n",
    "        sleep(5)\n",
    "    print(\"Sorry, url was nt donwloaded: {}\".format(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_queue(page_ind, game_ind):\n",
    "    page = get_page('https://gg.deals/games/?sort=metascore&type=1&page={}'.format(page_ind + 1))\n",
    "    \n",
    "    if page:\n",
    "        doc = lhtml.fromstring(page.text)\n",
    "        for game in doc.xpath('//a[contains(@class, \"game-link\")]/attribute::href', limit = game_ind):\n",
    "            queue.put(firstPart + game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_queue(num_pages = 300, games = 24):\n",
    "    block_numb = num_pages // games + 1\n",
    "    last_block = block_numb * games - num_pages\n",
    "\n",
    "    arg = zip(range(block_numb), [games] * (block_numb - 1) + [last_block])\n",
    "    with Pool(processes=10) as pool:\n",
    "        urls = pool.starmap(add_to_queue, arg)\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 312/312 [02:48<00:00,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "queue = Queue()\n",
    "create_queue()\n",
    "print(queue.qsize())\n",
    "\n",
    "# doc = lhtml.fromstring(ret.text)\n",
    "\n",
    "def process_page_wrapper(i):\n",
    "    with gzip.open('data/part_{:05d}.jsonl.gz'.format(i), mode='wb+') as f_json:\n",
    "        f_json = codecs.getwriter('utf8')(f_json)\n",
    "\n",
    "        while not queue.empty():\n",
    "            record = process_page(queue.get())\n",
    "            if (record == None):\n",
    "                break\n",
    "            record_str = json.dumps(record, ensure_ascii=False)\n",
    "            print(record_str, file=f_json)\n",
    "\n",
    "            # счетчик должен атомарно обновиться\n",
    "            with lock:\n",
    "                pbar.update(1)\n",
    "\n",
    "with Pool(processes=10) as pool, tqdm(total=queue.qsize()) as pbar:\n",
    "    lock = pbar.get_lock()\n",
    "    pool.map(process_page_wrapper, range(pool._processes))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
